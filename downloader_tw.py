# -*- coding: utf-8 -*-
import os
import time
import threading
import pandas as pd
import yfinance as yf
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from pathlib import Path

# ========== æ ¸å¿ƒåƒæ•¸è¨­å®š ==========
MARKET_CODE = "tw-share"
DATA_SUBDIR = "dayK"
PROJECT_NAME = "å°è‚¡æ—¥Kè³‡æ–™ä¸‹è¼‰å™¨"

# è·¯å¾‘è¨­å®šï¼šç¢ºä¿ç›¸å°æ–¼å°ˆæ¡ˆç›®éŒ„
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data", MARKET_CODE, DATA_SUBDIR)
LOG_DIR = os.path.join(BASE_DIR, "logs", PROJECT_NAME)
CKPT_FILE = os.path.join(LOG_DIR, "checkpoint_tw.csv")

MAX_WORKERS = 8      # å¤šåŸ·è¡Œç·’æ•¸é‡
MIN_FILE_SIZE = 100  # æœ‰æ•ˆæª”æ¡ˆæœ€å°ä½å…ƒçµ„
AUTO_ADJUST = False  # yfinance åƒ¹æ ¼èª¿æ•´

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
Path(DATA_DIR).mkdir(parents=True, exist_ok=True)
Path(LOG_DIR).mkdir(parents=True, exist_ok=True)

def log(msg: str):
    now = pd.Timestamp.now()
    log_path = os.path.join(LOG_DIR, f"download_tw_{now:%Y%m%d}.txt")
    with open(log_path, "a", encoding="utf-8-sig") as f:
        f.write(f"{now:%Y-%m-%d %H:%M:%S}: {msg}\n")
    print(msg)

def safe_filename(s: str) -> str:
    return (s.replace("/", "_").replace("\\", "_").replace(":", "_")
              .replace("*", "_").replace("?", "_").replace('"', "_")
              .replace("<", "_").replace(">", "_").replace("|", "_"))

def parse_item(item: str):
    """è§£æ 2330&å°ç©é›» æ ¼å¼"""
    if '&' in item:
        tkr, nm = item.split('&', 1)
    else:
        tkr, nm = item.strip(), "æœªçŸ¥è‚¡ç¥¨"
    return tkr.strip(), nm.strip()

def build_checkpoint(items):
    rows = []
    for it in items:
        tkr, nm = parse_item(it)
        out_path = os.path.join(DATA_DIR, f"{tkr}_{safe_filename(nm)}.csv")
        status = "skipped" if os.path.exists(out_path) and os.path.getsize(out_path) > MIN_FILE_SIZE else "pending"
        rows.append((tkr, nm, status, ""))
    df = pd.DataFrame(rows, columns=["ticker", "name", "status", "last_error"])
    df.to_csv(CKPT_FILE, index=False, encoding='utf-8-sig')
    return df

def download_stock_data(row):
    ticker_id, name = row["ticker"], row["name"]
    yf_ticker = ticker_id
    if ".TW" not in yf_ticker.upper() and ".TWO" not in yf_ticker.upper():
        yf_ticker = f"{ticker_id}.TW"

    try:
        out_path = os.path.join(DATA_DIR, f"{ticker_id}_{safe_filename(name)}.csv")
        
        # 1. æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
        if os.path.exists(out_path) and os.path.getsize(out_path) > MIN_FILE_SIZE:
            return {"ticker": ticker_id, "name": name, "status": "skipped", "err": "", "rows": 0}

        tk = yf.Ticker(yf_ticker)
        hist = tk.history(period="2y", auto_adjust=AUTO_ADJUST)

        # 2. é‡å°æŠ“ä¸åˆ°è³‡æ–™çš„ã€Œä¸‹å¸‚/ç„¡æ•ˆæ¨™çš„ã€è™•ç†
        if hist is None or hist.empty:
            # --- é—œéµå„ªåŒ–ï¼šå°‡å…¶æ¨™è¨˜ç‚º skippedï¼Œé€™æ¨£ä¸‹æ¬¡å°±ä¸æœƒå†æŠ“ ---
            return {"ticker": ticker_id, "name": name, "status": "skipped", "err": "delisted_or_empty", "rows": 0}

        hist.reset_index(inplace=True)
        hist.columns = [c.lower() for c in hist.columns]
        hist.to_csv(out_path, index=False, encoding='utf-8-sig')
        return {"ticker": ticker_id, "name": name, "status": "success", "err": "", "rows": len(hist)}

    except Exception as e:
        # å¦‚æœæ˜¯ç¶²è·¯éŒ¯èª¤ç­‰æ„å¤–ï¼Œæ‰æ¨™è¨˜ç‚º failed è®“ä¸‹æ¬¡é‡è©¦
        return {"ticker": ticker_id, "name": name, "status": "failed", "err": str(e), "rows": 0}
def get_full_stock_list():
    """
    é€™è£¡æ‡‰æ”¾å…¥ä½ åŸæœ¬æŠ“å– 2600 æª”å°è‚¡çš„çˆ¬èŸ²ä»£ç¢¼ã€‚
    æš«æ™‚ä»¥ç¤ºä¾‹ä»£æ›¿ï¼ŒåŸ·è¡Œå‰è«‹ç¢ºä¿æ­¤è™•å›å‚³å®Œæ•´æ¸…å–®ã€‚
    """
    # ç¤ºä¾‹ï¼šè«‹åœ¨æ­¤è®€å–ä½ çš„ CSV æ¸…å–®æˆ–åŸ·è¡Œçˆ¬èŸ²
    # return ["2330&å°ç©é›»", "2317&é´»æµ·", "0050&å…ƒå¤§å°ç£50"]
    return [] # <-- è¨˜å¾—æŠŠä½ çš„æ¸…å–®é‚è¼¯æ”¾é€²é€™è£¡

def main():
    """ä¾› main.py å‘¼å«çš„ä¸»é€²å…¥é»"""
    # é€™è£¡è«‹ç¢ºä¿ç²å–æ¸…å–®çš„é‚è¼¯æœ‰é‹ä½œ
    stockname_list = get_full_stock_list()
    
    # å¦‚æœæ¸…å–®æ˜¯ç©ºçš„ï¼Œå˜—è©¦å»è®€å–å·²æœ‰çš„ Checkpoint
    if not stockname_list and os.path.exists(CKPT_FILE):
        ckpt = pd.read_csv(CKPT_FILE)
        log(f"ğŸ” æ¸…å–®ç‚ºç©ºï¼Œè¼‰å…¥æ—¢æœ‰çºŒå‚³é»ï¼š{len(ckpt)} æª”")
    elif not stockname_list:
        log("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è‚¡ç¥¨æ¸…å–®ï¼Œè«‹æª¢æŸ¥ get_full_stock_list() å…§å®¹ã€‚")
        return
    else:
        if os.path.exists(CKPT_FILE):
            ckpt = pd.read_csv(CKPT_FILE)
            log(f"ğŸ” è¼‰å…¥çºŒå‚³é»ï¼š{len(ckpt)} æª”")
        else:
            ckpt = build_checkpoint(stockname_list)
            log("ğŸ†• å»ºç«‹æ–°çºŒå‚³é»")

    todo = ckpt[ckpt["status"].isin(["pending", "failed"])].copy()
    
    if len(todo) == 0:
        log("ğŸ‰ å°è‚¡æ•¸æ“šå·²å°±ç·’ï¼Œç„¡éœ€ä¸‹è¼‰ã€‚")
        return

    log(f"ğŸš€ é–‹å§‹ä¸‹è¼‰ {len(todo)} æ”¯æ¨™çš„...")
    results = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_row = {executor.submit(download_stock_data, r): r for _, r in todo.iterrows()}
        pbar = tqdm(total=len(todo), desc="å°è‚¡ä¸‹è¼‰é€²åº¦")
        
        for future in as_completed(future_to_row):
            result = future.result()
            results.append(result)
            
            # å³æ™‚æ›´æ–° Checkpoint ç‹€æ…‹
            mask = (ckpt["ticker"] == result["ticker"])
            ckpt.loc[mask, ["status", "last_error"]] = [result["status"], result["err"]]
            ckpt.to_csv(CKPT_FILE, index=False, encoding='utf-8-sig')
            
            pbar.update(1)
        pbar.close()

    success_count = len([r for r in results if r['status']=='success'])
    log(f"ğŸ“Š åŸ·è¡Œå®Œç•¢ã€‚æˆåŠŸä¸‹è¼‰: {success_count} æ”¯æ¨™çš„")

if __name__ == "__main__":
    main()